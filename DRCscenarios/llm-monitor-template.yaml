
apiVersion: ai.sap.com/v1alpha1
kind: ServingTemplate
metadata:
  name: llm-monitor-serving # executable ID, must be unique across your SAP AI Core instance, for example use `server-pipeline-yourname-1234`
  annotations:  
    scenarios.ai.sap.com/description: "llm monitor service"
    scenarios.ai.sap.com/name: "llm monitor service" # Scenario name should be the use case
    executables.ai.sap.com/description: "llm monitor service Completions"
    executables.ai.sap.com/name: "llm-monitor-service" # Executable name should describe the workflow in the use case
  labels:
    scenarios.ai.sap.com/id: "llm-monitor-service"
    ai.sap.com/version: "4.0"
    ext.ai.sap.com/islm_released_version: "true"
    ext.ai.sap.com/islm_executable_type: "inference"
    ext.ai.sap.com/islm_inference_type: "online"
spec:
  imagePullSecrets: # (optional if using a private docker hub repository)
   - name: credstutorialrepo # your docker registry secret
  template:
    apiVersion: "serving.kserve.io/v1beta1"
    metadata:
      annotations: |
        autoscaling.knative.dev/metric: concurrency   # condition when to scale
        autoscaling.knative.dev/target: 1
        autoscaling.knative.dev/targetBurstCapacity: 0
      labels: |
        ai.sap.com/resourcePlan: starter # computing power
    spec: |
      predictor:
        minReplicas: 1
        maxReplicas: 1    # how much to scale
        containers:
        - name: kserve-container
          image: docker.io/rotin/llm-monitor-service:02
          ports:
            - containerPort: 9001    # customizable port
              protocol: TCP
          command: ["/bin/sh", "-c"]
          args:
            - >
              set -e && echo "Starting" && gunicorn --chdir /app/src main:app -b 0.0.0.0:9001 # filename `main` flask variable `app`
